<style>
    html {
        font-family: Roboto
    }
</style>

Jared Rackley

Artificial intelligence has become a popular buzzword among popular culture today. However, this has also been a big factor in spreading misinformation about AI applications, with movies spreading propaganda of a future run by it. However, quantum computing has also been a field of interest, with great possibilities in the future. These two fields of computer science are very powerful and can both stand on their own. However, even greater possibilities arise when they are combined!

What AI can and cannot do

Although popular media has portrayed artificial intelligence as a powerful tool that dominates the future, there are limits to its capabilities. Neural networks are meant to simulate brains, where each node is a neuron. However, artificial intelligence has not reached the point where it can continuously learn and adapt, nor use common sense (1). The artificial intelligence that we know today comes from taking a dataset with predetermined inputs and then training the model to determine the best possible output or behavior. 

Most artificial intelligence used in organizations and companies today is very specific. It can tell you who's most likely to order a product based on the item and their profile, what TV shows and movies you probably want to watch (Netflix does this), interpret and give results for things you ask for (voice assistants like alexa and siri), and many other things! This application of artificial intelligence won't work for any problem, as it's only programmed to work for a specific topic and criteria. 

The other type of AI that is much harder to develop, meaning it cannot be used, because so far, it hasn't been realized, is a general (or bottom-up) AI. The reason why this type of AI is so difficult is because of the constraints of a machine that humans don't have. A famous scientist named Gödel came up with a formula saying that if we have a formal arithmetic system F, it is impossible to construct a computer which will output all theorems of F. He showed this by giving the example of the Gödel Sentence G which is in F, that states "I am not a theorem of F." If this theorem is true, then it leads to a contradiction in F. However, if it is not true, then F is incomplete, because it cannot be proved. (4) 

This example proved that machines have constraints that humans don't. Gödel was able to see the flaw with the arithmetic system F, and humans can account for this, while no machine could account for this, because it itself exists within these constraints. 

This isn't to say that general AI cannot be achieved. In fact, another scientist named Schmidhuber developed the Gödel machine, which uses a recursive strategy to self-improve (4). Still, we have not been able to develop an AI system that is capable of self-developing, other than very specific situations. Branislav Holländer, who wrote an article on this topic, stated this:
<p>
    “The problem with [an AI that develops without human intervention], however,
is that <strong>we actually did not succeed in constructing any self-improving
AI</strong>. Today’s machine learning algorithms are capable of modifying
themselves to a certain degree, e.g. one-shot learning algorithms, but it is not
nearly at the level of human development. <strong>Genetic algorithms</strong>
may be able to self-improve by mechanisms similar to biological life, but they
usually require a fair amount of fine-tuning by humans to work correctly. A
self-improving AI on the other hand, would only require the intelligence of a
newborn at first and could develop itself from that point on to any level of
intelligence required.”
</p>


Read more about Why We May Never Develop General AI: https://towardsdatascience.com/why-we-may-never-develop-strong-ai-e49c805480f

However, this does not mean that AI cannot do fantastic things. In 2020, DeepMind's AI solved the protein-folding problem, which had been baffling Scientists for 50 years (2). Other examples include virtual assistants, which many people had thought were impossible, but now exist in most smartphones, and automated vehicles, which for the drive better than most humans in general situations. AI still continues to develop, and it continues to have technological breakthroughs!

What is Quantum Computing

The idea of quantum computing was first proposed by Richard Feynman in 1982. He wanted to apply quantum mechanics to the growing field of computing. His ideas were put into a research paper that introduced the idea of a quantum turing machine. After that, more scientists began to come up with quantum solutions to problems in computing. Solutions to well-known computer science problems went from being exponential in time to only taking polynomial time. The major reason for this is because of quantum parallelism (3). Qubits are quantum bits, meaning at any point in time, they can be a zero, one, or a superposition of both zero and one. This superposition has a drastic effect on the amount of processing, because it means that when we are processing bits, we can process all the bits at the same time. 

Read More about quantum bits and superposition: https://cosmosmagazine.com/science/quantum-computing-for-the-qubit-curious/

Because of this, when we put quantum bits into a quantum register (a register that holds quantum bits) with a capacity of n bits, we can calculate all possible values of an algorithm with length n with astounding time differences. The possibilities are endless!

Quantum Applications in AI

References:
<ol>
<li><code><a
href="https://www.forbes.com/sites/robtoews/2021/06/01/what-artificial-intelligence-still-cant-do/?sh=66c1a3e866f6">https://www.forbes.com/sites/robtoews/2021/06/01/what-artificial-intelligence-still-cant-do/?sh=66c1a3e866f6</a></code>
<li><code><a
href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology">https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology</a></code>
<li><code><a
href="https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0004370209001398/pdf%3Fmd5%3Ddf7abb8c5f73363db0c3d5297b810831%26pid%3D1-s2.0-S0004370209001398-main.pdf&hl=en&sa=X&ei=PYmkYb_QLpeP6rQP75KZsAg&scisig=AAGBfm3boHl-Ugt2dbd04vC1HUSKTKAUuQ&oi=scholarr">https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0004370209001398/pdf%3Fmd5%3Ddf7abb8c5f73363db0c3d5297b810831%26pid%3D1-s2.0-S0004370209001398-main.pdf&hl=en&sa=X&ei=PYmkYb_QLpeP6rQP75KZsAg&scisig=AAGBfm3boHl-Ugt2dbd04vC1HUSKTKAUuQ&oi=scholarr</a></code>
<li><code><a
href="https://towardsdatascience.com/why-we-may-never-develop-strong-ai-e49c805480f">https://towardsdatascience.com/why-we-may-never-develop-strong-ai-e49c805480f</a></code>
<li>
</li>
</ol>


<pre
class="prettyprint">Image Links:
    https://toptenscience.com/wp-content/uploads/2019/04/artificial-intelligence.jpg
</pre>
